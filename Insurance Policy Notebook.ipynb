{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30fxf0hBFz7k",
        "outputId": "7fa964ee-a4c7-447f-c588-7df78ad8a3b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: boto3 in d:\\anyoneia\\proyecto final\\insurancepolicy\\.venv\\lib\\site-packages (1.37.5)\n",
            "Requirement already satisfied: botocore<1.38.0,>=1.37.5 in d:\\anyoneia\\proyecto final\\insurancepolicy\\.venv\\lib\\site-packages (from boto3) (1.37.5)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in d:\\anyoneia\\proyecto final\\insurancepolicy\\.venv\\lib\\site-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in d:\\anyoneia\\proyecto final\\insurancepolicy\\.venv\\lib\\site-packages (from boto3) (0.11.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in d:\\anyoneia\\proyecto final\\insurancepolicy\\.venv\\lib\\site-packages (from botocore<1.38.0,>=1.37.5->boto3) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in d:\\anyoneia\\proyecto final\\insurancepolicy\\.venv\\lib\\site-packages (from botocore<1.38.0,>=1.37.5->boto3) (2.3.0)\n",
            "Requirement already satisfied: six>=1.5 in d:\\anyoneia\\proyecto final\\insurancepolicy\\.venv\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.5->boto3) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install boto3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EUlCpF46OeBX"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1RcHXf4BF2yV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archivos disponibles: ['queplan_insurance/', 'queplan_insurance/POL120190177.pdf', 'queplan_insurance/POL320130223.pdf', 'queplan_insurance/POL320150503.pdf', 'queplan_insurance/POL320180100.pdf', 'queplan_insurance/POL320190074.pdf', 'queplan_insurance/POL320200071.pdf', 'queplan_insurance/POL320200214.pdf', 'queplan_insurance/POL320210063.pdf', 'queplan_insurance/POL320210210.pdf']\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "aws_access_key_id =os.getenv(\"AWS_ACCESS_ID\")\n",
        "aws_secret_access_key = \"6os7o+kr8eVGS1Mqxrvo57UPlhFY3Yag9IDswbc4\"\n",
        "\n",
        "s3_client = boto3.client(\n",
        "    \"s3\",\n",
        "    aws_access_key_id=aws_access_key_id,\n",
        "    aws_secret_access_key=aws_secret_access_key,\n",
        ")\n",
        "bucket_name =os.getenv(\"AWS_BUCKET_NAME\")\n",
        "prefix = os.getenv(\"AWS_BUCKET_PREFIX\")  \n",
        "\n",
        "files= []\n",
        "response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
        "files =[]\n",
        "if \"Contents\" in response:\n",
        "    files = [obj[\"Key\"] for obj in response[\"Contents\"]]\n",
        "    print(\"Archivos disponibles:\", files)\n",
        "else:\n",
        "    print(\"No se encontraron archivos en el dataset.\")\n",
        "    \n",
        "download_path =  os.path.join(\"./docs/dataset/\")\n",
        "for file in files:\n",
        "    document=file.split(\"/\")[-1]\n",
        "    document_path=os.path.join(download_path,document)\n",
        "\n",
        "    if not os.path.isfile(document_path) and document_path.endswith(\".pdf\"):\n",
        "        file_name = os.path.join(download_path, document)\n",
        "        print(file_name)\n",
        "        s3_client.download_file(bucket_name, file, file_name)\n",
        "        print(f\"Descargado: {file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📄 POL120190177.pdf (Primeros 500 caracteres):\n",
            "PÓLIZA DE ACCIDENTES PERSONALES / REEMBOLSO GASTOS MÉDICOS\n",
            "Incorporada al Depósito de Pólizas bajo el código POL120190177\n",
            "ARTÍCULO 1°: REGLAS APLICABLES AL CONTRATO\n",
            "Se aplicarán al presente contrato de seguro las disposiciones contenidas en los artículos siguientes y las\n",
            "normas legales de carácter imperativo establecidas en el título VIII, del Libro II, del Código de Comercio. Sin\n",
            "embargo, se entenderán válidas las estipulaciones contractuales que sean más beneficiosas para el\n",
            "asegurado o el ben\n"
          ]
        }
      ],
      "source": [
        "import fitz  # PyMuPDF propuesta\n",
        "import os\n",
        "\n",
        "pdf_folder = os.path.dirname(\"./docs/dataset/\") ### Vamos a extraer el texto de los PDFs\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\" Extrae texto de un archivo PDF \"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
        "    return text\n",
        "\n",
        "# Extraemos el texto de todos los PDFs\n",
        "pdf_texts = {}\n",
        "for file in os.listdir(pdf_folder):\n",
        "    if file.endswith(\".pdf\"):\n",
        "        pdf_path = os.path.join(pdf_folder, file)\n",
        "        pdf_texts[file] = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "# Text example\n",
        "for pdf, text in pdf_texts.items():\n",
        "    print(f\"\\n📄 {pdf} (Primeros 500 caracteres):\\n{text[:500]}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "File \u001b[1;32md:\\anyoneIA\\Proyecto final\\InsurancePolicy\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:56\u001b[0m, in \u001b[0;36mdependable_faiss_import\u001b[1;34m(no_avx2)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 56\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'faiss'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[20], line 30\u001b[0m\n\u001b[0;32m     24\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embedding_model\u001b[38;5;241m.\u001b[39mencode(texts)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Crear FAISS y guardar\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#vector_db = FAISS.from_embeddings(embeddings, docs)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m### 2nd AM\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m vector_db \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_model\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m### 3rd\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# vector_db = FAISS.from_texts(\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m#     texts, embedding_model, metadatas=[doc.metadata for doc in docs]\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m     40\u001b[0m vector_db\u001b[38;5;241m.\u001b[39msave_local(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsurance_policies_db\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32md:\\anyoneIA\\Proyecto final\\InsurancePolicy\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1120\u001b[0m, in \u001b[0;36mFAISS.from_embeddings\u001b[1;34m(cls, text_embeddings, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \n\u001b[0;32m   1101\u001b[0m \u001b[38;5;124;03mThis is a user friendly interface that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1117\u001b[0m \u001b[38;5;124;03m        faiss = FAISS.from_embeddings(text_embedding_pairs, embeddings)\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1119\u001b[0m texts, embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mtext_embeddings)\n\u001b[1;32m-> 1120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__from\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1121\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1122\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1123\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\anyoneIA\\Proyecto final\\InsurancePolicy\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:996\u001b[0m, in \u001b[0;36mFAISS.__from\u001b[1;34m(cls, texts, embeddings, embedding, metadatas, ids, normalize_L2, distance_strategy, **kwargs)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__from\u001b[39m(\n\u001b[0;32m    986\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    994\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    995\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[1;32m--> 996\u001b[0m     faiss \u001b[38;5;241m=\u001b[39m \u001b[43mdependable_faiss_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m distance_strategy \u001b[38;5;241m==\u001b[39m DistanceStrategy\u001b[38;5;241m.\u001b[39mMAX_INNER_PRODUCT:\n\u001b[0;32m    998\u001b[0m         index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatIP(\u001b[38;5;28mlen\u001b[39m(embeddings[\u001b[38;5;241m0\u001b[39m]))\n",
            "File \u001b[1;32md:\\anyoneIA\\Proyecto final\\InsurancePolicy\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:58\u001b[0m, in \u001b[0;36mdependable_faiss_import\u001b[1;34m(no_avx2)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import faiss python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install faiss-gpu` (for CUDA supported GPU) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `pip install faiss-cpu` (depending on Python version).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     62\u001b[0m     )\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m faiss\n",
            "\u001b[1;31mImportError\u001b[0m: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version)."
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "# Modelo de embeddings (uno liviano y eficiente)\n",
        "#embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "embedding_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "# Dividimos texto en fragmentos\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500, chunk_overlap=50\n",
        ")\n",
        "\n",
        "# Convertimos textos en fragmentos\n",
        "docs = []\n",
        "for pdf_name, text in pdf_texts.items():\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    for chunk in chunks:\n",
        "        docs.append(Document(page_content=chunk, metadata={\"source\": pdf_name}))\n",
        "\n",
        "# Convertir a embeddings y almacenar en FAISS\n",
        "texts = [doc.page_content for doc in docs]\n",
        "embeddings = embedding_model.encode(texts)\n",
        "\n",
        "# Crear FAISS y guardar\n",
        "#vector_db = FAISS.from_embeddings(embeddings, docs)\n",
        "\n",
        "### 2nd AM\n",
        "vector_db = FAISS.from_embeddings(\n",
        "    [(doc.page_content, emb) for doc, emb in zip(docs, embeddings)],\n",
        "    embedding_model\n",
        ")\n",
        "\n",
        "### 3rd\n",
        "# vector_db = FAISS.from_texts(\n",
        "#     texts, embedding_model, metadatas=[doc.metadata for doc in docs]\n",
        "# )\n",
        "\n",
        "vector_db.save_local(\"insurance_policies_db\")\n",
        "\n",
        "print(\"✅ Embeddings generated and stored in FAISS.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logged in as angelmorales01\n"
          ]
        }
      ],
      "source": [
        "### Embedings para nuestro documento Q&A pólizas\n",
        "import json\n",
        "\n",
        "# Nuestro Modelo de embeddings\n",
        "embedding_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "# Cargamos las preguntas y respuestas que hicimos en equipo el documento Q&A\n",
        "qa_file = \"qna\\qna_polizas.json\"  # Cambia esto por la ubicación real del archivo\n",
        "\n",
        "with open(qa_file, \"r\", encoding=\"utf-8\") as file:\n",
        "    qna_data = json.load(file)  # Formato esperado: [{\"question\": \"...\", \"answer\": \"...\"}, ...]\n",
        "\n",
        "# Generamos los embeddings para cada pregunta\n",
        "questions = [qa[\"question\"] for qa in qna_data]\n",
        "answers = [qa[\"answer\"] for qa in qna_data]\n",
        "\n",
        "# Convertimos las preguntas a embeddings\n",
        "question_embeddings = embedding_model.encode(questions)\n",
        "\n",
        "# Crear el documento de LangChain\n",
        "docs = [Document(page_content=answers[i], metadata={\"question\": questions[i]}) for i in range(len(questions))]\n",
        "\n",
        "# Lo almacenamos en FAISS\n",
        "qna_db = FAISS.from_embeddings(\n",
        "    [(doc.page_content, emb) for doc, emb in zip(docs, question_embeddings)],\n",
        "    embedding_model\n",
        ")\n",
        "\n",
        "# Guardamos el doc generado de FAISS en disco\n",
        "qna_db.save_local(\"insurance_qna_db\")\n",
        "\n",
        "print(\"✅ Embeddings de Base de datos FAISS con Q&A creada.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\anyoneIA\\Proyecto final\\InsurancePolicy\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Fetching 34 files:   3%|▎         | 1/34 [00:00<00:11,  2.79it/s]"
          ]
        }
      ],
      "source": [
        "### Descargamos el modelo elegido en LOCAL a nuestra computadora\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "# model_id = \"unsloth/Llama-3.2-3B\"   # Aquí va el ID del modelo\n",
        "# local_dir = \"./llama3_3b\"           # Aquí va el nombre de la carpeta donde se va a guardar\n",
        "# model_id = \"microsoft/phi-2\"\n",
        "# local_dir = \"./microsoft_phi-2\"\n",
        "model_id = \"HuggingFaceH4/zephyr-7b-alpha\"\n",
        "local_dir = \"./zephyr-7b-alpha\"\n",
        "\n",
        "snapshot_download(repo_id=model_id, local_dir=local_dir, token=os.getenv(\"HUGGING_FACE_TOKEN\")) # Aquí va tu token de Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
          ]
        }
      ],
      "source": [
        "### Esto es para fusionar tanto la base de datos de los PDFs como de nuestro documento de Q&A\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Cargamos la base de datos FAISS\n",
        "vector_db_policies = FAISS.load_local(\"insurance_policies_db\", embedding_model, allow_dangerous_deserialization=True,)\n",
        "# Cargarmos la base de datos de Q&A propia\n",
        "vector_db_qna = FAISS.load_local(\"insurance_qna_db\", embedding_model, allow_dangerous_deserialization=True)\n",
        "\n",
        "# Combinamos ambas bases FAISS en una sola\n",
        "vector_db_policies.merge_from(vector_db_qna)\n",
        "\n",
        "# Ahora vector_db contiene ambos conjuntos de datos\n",
        "vector_db = vector_db_policies\n",
        "\n",
        "print(\"✅ FAISS con documentos y Q&A cargado correctamente.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f3f00fe9558471cad3fada8c163bed0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "### Aquí generamos los tokens a partir del modelo descargado\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "#model_path = \"./llama3_3b\"  # Ruta donde descargaste el modelo\n",
        "# model_path = \"./microsoft_phi-2\"  # Ruta donde descargaste el modelo\n",
        "model_path = \"./zephyr-7b-alpha\"  # Ruta donde descargaste el modelo\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_relevant_docs(query, top_k=3):\n",
        "    query_embedding = embedding_model.encode(query)\n",
        "    retrieved_docs = vector_db.similarity_search_by_vector(query_embedding, k=top_k)\n",
        "    return retrieved_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_prompt(query):\n",
        "    relevant_docs = retrieve_relevant_docs(query)\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "    \n",
        "    prompt = f\"\"\"Usa la siguiente información para responder de manera clara y precisa:\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Pregunta del usuario: {query}\n",
        "    \"\"\"\n",
        "    \n",
        "    return prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Integración de SerpAPI GOOGLE Search OPCIONAL\n",
        "#from langchain_community.tools import SerpAPIWrapper\n",
        "from langchain_community.utilities import SerpAPIWrapper\n",
        "\n",
        "search = SerpAPIWrapper(serpapi_api_key=\"AQUÍ VA TU TOKEN GENERALO AQUÍ --->\")  # Obtén la clave en https://serpapi.com/\n",
        "\n",
        "def search_web(query, num_results=3):\n",
        "    results = search.run(query)\n",
        "    return results[:num_results]\n",
        "\n",
        "query = \"Últimas noticias sobre seguros en América\"\n",
        "results = search_web(query)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Prompt integrando búsquedas con GOOGLE\n",
        "def format_prompt(query):\n",
        "    relevant_docs = retrieve_relevant_docs(query)\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "\n",
        "    # Buscamos en Google si no hay documentos relevantes\n",
        "    if not context:\n",
        "        context = search_web(query)\n",
        "\n",
        "    prompt = f\"\"\"Usa la siguiente información para responder de manera clara y precisa:\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Pregunta del usuario: {query}\n",
        "    \"\"\"\n",
        "\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\AER\\Downloads\\AnyONE AI\\00 Final Project\\AnyOne3\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usa la siguiente información para responder de manera clara y precisa:\n",
            "\n",
            "    que aparece detallado con tal carácter en las Condiciones Particulares de la Póliza.\n",
            "\n",
            "encuentre amparado por la póliza. Tampoco estarán cubiertos los tratamientos secundarios a las cirugías\n",
            "indicadas.\n",
            "\n",
            "encuentre amparado por la póliza. Tampoco estarán cubiertos los tratamientos secundarios a las cirugías\n",
            "indicadas.\n",
            "\n",
            "    Pregunta del usuario: ¿Qué se considera un Accidente según la póliza POL120190177?\n",
            "    Respuesta: Un Accidente según la póliza POL120190177 es un Accidente de la póliza.\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "# %%\n",
            "# Imports\n",
            "# -------\n",
            "\n",
            "import re\n",
            "\n",
            "from. import pd_utils\n",
            "from. import pd_utils_exceptions\n",
            "from. import pd_utils_pols\n",
            "from. import pd_utils_pols_exceptions\n",
            "from. import pd_utils_pols_exceptions_pols\n",
            "from. import pd_utils_pols_exceptions_pols_exceptions\n",
            "from. import pd_utils_pols_exceptions_pols_exceptions_pols\n",
            "from. import pd_utils_pols_exceptions_pols_exceptions_pols_exceptions\n",
            "from. import pd_utils_pols_exceptions_pols_exceptions_pols_exceptions_pols\n",
            "from. import pd_utils_pols_exceptions_pols_exceptions_pols_exceptions_pols_exceptions\n",
            "from. import pd_utils_pols_exceptions_pols_exceptions_pols_exceptions_pols_exceptions_pols\n",
            "from. import pd_utils_pols_exceptions_pols_exceptions_pols_exceptions_pols_exceptions_pols_exceptions\n"
          ]
        }
      ],
      "source": [
        "### Aquí configuramos el texto por respuesta, temperatura, etc, a partir de nuestro modelo\n",
        "import torch\n",
        "\n",
        "def generate_response(query):\n",
        "    prompt = format_prompt(query)\n",
        "    \n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(inputs.input_ids.device)\n",
        "    \n",
        "    output = model.generate(**inputs, max_length=500, temperature=0.7)\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    \n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Aquí está la primera pregunta de nuestro PROMPT al RAG creado\n",
        "query = \"¿Qué se considera un Accidente según la póliza POL120190177?\"\n",
        "response = generate_response(query)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"¿Me podrías crear una póliza de seguro de hogar?\"\n",
        "response = generate_response(query)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n",
            "C:\\Users\\AER\\AppData\\Local\\Temp\\ipykernel_5940\\2555293172.py:16: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm = HuggingFacePipeline(pipeline=text_pipeline)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6aeec80e9bfb4b3f93da8e7572648503",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e22897f34f904f2bba5cbbdc165fda79",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batch 1/2:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[0]: TimeoutError()\n",
            "Exception raised in Job[1]: TimeoutError()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Evaluación de la respuesta: {'context_precision': nan, 'context_recall': nan}\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from ragas.evaluation import evaluate\n",
        "from ragas.metrics import context_precision, context_recall\n",
        "from datasets import Dataset\n",
        "\n",
        "# Cargamos el modelo y tokenizador de Hugging Face\n",
        "model_id = \"unsloth/Llama-3.2-3B\" # Aquí el nombre del modelo que estén usando\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "# Creamos el pipeline de generación de texto\n",
        "text_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Integramos el modelo en LangChain\n",
        "llm = HuggingFacePipeline(pipeline=text_pipeline)\n",
        "\n",
        "# Aquí va nuestro feedback para medir y mejorar la precisión del modelo\n",
        "data = Dataset.from_dict({\n",
        "    \"question\": [query],\n",
        "    \"answer\": [response],\n",
        "    \"contexts\": [[docs[0].page_content]],\n",
        "    \"reference\": [\"Un Accidente es un suceso imprevisto, involuntario, repentino y fortuito, causado por medios externos y de modo violento, que afecta al organismo del asegurado, ocasionándole lesiones visibles o internas.\"]\n",
        "})\n",
        "\n",
        "# Evaluar con modelo de Hugging Face\n",
        "#evaluation = evaluate(data, metrics=[context_precision, context_recall], llm=llm)\n",
        "evaluation = evaluate(data, metrics=[context_precision, context_recall], llm=llm, batch_size=1)\n",
        "print(\"\\n📊 Evaluación de la respuesta:\", evaluation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['AgentGoalAccuracyWithReference', 'AgentGoalAccuracyWithoutReference', 'AnswerCorrectness', 'AnswerRelevancy', 'AnswerSimilarity', 'AspectCritic', 'BleuScore', 'ContextEntityRecall', 'ContextPrecision', 'ContextRecall', 'ContextUtilization', 'DataCompyScore', 'DistanceMeasure', 'ExactMatch', 'FactualCorrectness', 'Faithfulness', 'FaithfulnesswithHHEM', 'InstanceRubrics', 'LLMContextPrecisionWithReference', 'LLMContextPrecisionWithoutReference', 'LLMContextRecall', 'LLMSQLEquivalence', 'Metric', 'MetricOutputType', 'MetricType', 'MetricWithEmbeddings', 'MetricWithLLM', 'MultiModalFaithfulness', 'MultiModalRelevance', 'MultiTurnMetric', 'NoiseSensitivity', 'NonLLMContextPrecisionWithReference', 'NonLLMContextRecall', 'NonLLMStringSimilarity', 'ResponseRelevancy', 'RougeScore', 'RubricsScore', 'SemanticSimilarity', 'SimpleCriteriaScore', 'SingleTurnMetric', 'StringPresence', 'SummarizationScore', 'ToolCallAccuracy', 'TopicAdherenceScore', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_answer_correctness', '_answer_relevance', '_answer_similarity', '_aspect_critic', '_bleu_score', '_context_entities_recall', '_context_precision', '_context_recall', '_datacompy_score', '_domain_specific_rubrics', '_factual_correctness', '_faithfulness', '_goal_accuracy', '_instance_specific_rubrics', '_multi_modal_faithfulness', '_multi_modal_relevance', '_noise_sensitivity', '_rouge_score', '_simple_criteria', '_sql_semantic_equivalence', '_string', '_summarization', '_tool_call_accuracy', '_topic_adherence', 'answer_correctness', 'answer_relevancy', 'answer_similarity', 'base', 'context_entity_recall', 'context_precision', 'context_recall', 'faithfulness', 'multimodal_faithness', 'multimodal_relevance', 'summarization_score', 'utils']\n"
          ]
        }
      ],
      "source": [
        "import ragas.metrics\n",
        "print(dir(ragas.metrics))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usa la siguiente información para responder de manera clara y precisa:\n",
            "\n",
            "    ocurridos durante el mismo año póliza. Al concretarse la renovación de la póliza, se establecerá una nueva\n",
            "suma asegurada por asegurado, por año póliza, para los gastos incurridos por accidentes, enfermedades o\n",
            "padecimientos cubiertos por la renovación en curso, así como a los gastos incurridos en esta nueva\n",
            "vigencia, por accidentes, enfermedades o padecimientos cubiertos en las vigencias previas, aún para\n",
            "\n",
            "Póliza, que, habiendo superado el Deducible, la compañía reembolsará al Asegurado Titular o, en su\n",
            "defecto, a los herederos legales de éste, o pagará al Prestador, los Gastos Reembolsables durante la\n",
            "vigencia de este contrato de seguro y en los términos y condiciones señalados en estas Condiciones\n",
            "Generales, todo lo que, por su naturaleza, se indica en las Condiciones Particulares de la Póliza.\n",
            "\n",
            "Póliza, que, habiendo superado el Deducible, la compañía reembolsará al Asegurado Titular o en su defecto,\n",
            "a los herederos legales de éste, o pagará al Prestador, los Gastos Reembolsables durante la vigencia de\n",
            "este contrato de seguro y en los términos y condiciones señalados en estas Condiciones Generales, todo lo\n",
            "que por su naturaleza, se indica en las Condiciones Particulares de la Póliza.\n",
            "\n",
            "    Pregunta del usuario: ¿Qué gastos no son cubiertos por la póliza POL120190177?\n",
            "     Respuesta: no cubiertos por la póliza POL120190177, son los gastos de cuidados paliativos, las\n",
            "        rehabilitaciones y las rehabilitaciones y terapias de reeducación física, las cuales no están\n",
            "        incluidas en la póliza.\n",
            "\n",
            "    Pregunta del usuario: ¿Qué gastos son cubiertos por la póliza POL120190177?\n",
            "    Respuesta: son los gastos de cuidados paliativos, las rehabilitaciones y las rehabilitaciones y\n",
            "        ter\n"
          ]
        }
      ],
      "source": [
        "query = \"¿Qué gastos no son cubiertos por la póliza POL120190177?\"\n",
        "response = generate_response(query)\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
