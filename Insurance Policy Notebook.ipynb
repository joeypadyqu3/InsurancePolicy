{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30fxf0hBFz7k",
        "outputId": "7fa964ee-a4c7-447f-c588-7df78ad8a3b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: boto3 in d:\\anyoneia\\proyecto final\\insurancepolicy\\.venv\\lib\\site-packages (1.37.5)\n",
            "Requirement already satisfied: botocore<1.38.0,>=1.37.5 in d:\\anyoneia\\proyecto final\\insurancepolicy\\.venv\\lib\\site-packages (from boto3) (1.37.5)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in d:\\anyoneia\\proyecto final\\insurancepolicy\\.venv\\lib\\site-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in d:\\anyoneia\\proyecto final\\insurancepolicy\\.venv\\lib\\site-packages (from boto3) (0.11.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in d:\\anyoneia\\proyecto final\\insurancepolicy\\.venv\\lib\\site-packages (from botocore<1.38.0,>=1.37.5->boto3) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in d:\\anyoneia\\proyecto final\\insurancepolicy\\.venv\\lib\\site-packages (from botocore<1.38.0,>=1.37.5->boto3) (2.3.0)\n",
            "Requirement already satisfied: six>=1.5 in d:\\anyoneia\\proyecto final\\insurancepolicy\\.venv\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.5->boto3) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install boto3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EUlCpF46OeBX"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1RcHXf4BF2yV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archivos disponibles: ['queplan_insurance/', 'queplan_insurance/POL120190177.pdf', 'queplan_insurance/POL320130223.pdf', 'queplan_insurance/POL320150503.pdf', 'queplan_insurance/POL320180100.pdf', 'queplan_insurance/POL320190074.pdf', 'queplan_insurance/POL320200071.pdf', 'queplan_insurance/POL320200214.pdf', 'queplan_insurance/POL320210063.pdf', 'queplan_insurance/POL320210210.pdf']\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "aws_access_key_id =os.getenv(\"AWS_ACCESS_ID\")\n",
        "aws_secret_access_key = \"6os7o+kr8eVGS1Mqxrvo57UPlhFY3Yag9IDswbc4\"\n",
        "\n",
        "s3_client = boto3.client(\n",
        "    \"s3\",\n",
        "    aws_access_key_id=aws_access_key_id,\n",
        "    aws_secret_access_key=aws_secret_access_key,\n",
        ")\n",
        "bucket_name =os.getenv(\"AWS_BUCKET_NAME\")\n",
        "prefix = os.getenv(\"AWS_BUCKET_PREFIX\")  \n",
        "\n",
        "files= []\n",
        "response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
        "files =[]\n",
        "if \"Contents\" in response:\n",
        "    files = [obj[\"Key\"] for obj in response[\"Contents\"]]\n",
        "    print(\"Archivos disponibles:\", files)\n",
        "else:\n",
        "    print(\"No se encontraron archivos en el dataset.\")\n",
        "    \n",
        "download_path =  os.path.join(\"./docs/dataset/\")\n",
        "for file in files:\n",
        "    document=file.split(\"/\")[-1]\n",
        "    document_path=os.path.join(download_path,document)\n",
        "\n",
        "    if not os.path.isfile(document_path) and document_path.endswith(\".pdf\"):\n",
        "        file_name = os.path.join(download_path, document)\n",
        "        print(file_name)\n",
        "        s3_client.download_file(bucket_name, file, file_name)\n",
        "        print(f\"Descargado: {file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìÑ POL120190177.pdf (Primeros 500 caracteres):\n",
            "P√ìLIZA DE ACCIDENTES PERSONALES / REEMBOLSO GASTOS M√âDICOS\n",
            "Incorporada al Dep√≥sito de P√≥lizas bajo el c√≥digo POL120190177\n",
            "ART√çCULO 1¬∞: REGLAS APLICABLES AL CONTRATO\n",
            "Se aplicar√°n al presente contrato de seguro las disposiciones contenidas en los art√≠culos siguientes y las\n",
            "normas legales de car√°cter imperativo establecidas en el t√≠tulo VIII, del Libro II, del C√≥digo de Comercio. Sin\n",
            "embargo, se entender√°n v√°lidas las estipulaciones contractuales que sean m√°s beneficiosas para el\n",
            "asegurado o el ben\n"
          ]
        }
      ],
      "source": [
        "import fitz  # PyMuPDF propuesta\n",
        "import os\n",
        "\n",
        "pdf_folder = os.path.dirname(\"./docs/dataset/\") ### Vamos a extraer el texto de los PDFs\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\" Extrae texto de un archivo PDF \"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
        "    return text\n",
        "\n",
        "# Extraemos el texto de todos los PDFs\n",
        "pdf_texts = {}\n",
        "for file in os.listdir(pdf_folder):\n",
        "    if file.endswith(\".pdf\"):\n",
        "        pdf_path = os.path.join(pdf_folder, file)\n",
        "        pdf_texts[file] = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "# Text example\n",
        "for pdf, text in pdf_texts.items():\n",
        "    print(f\"\\nüìÑ {pdf} (Primeros 500 caracteres):\\n{text[:500]}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version).",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "File \u001b[1;32md:\\anyoneIA\\Proyecto final\\InsurancePolicy\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:56\u001b[0m, in \u001b[0;36mdependable_faiss_import\u001b[1;34m(no_avx2)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 56\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'faiss'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[20], line 30\u001b[0m\n\u001b[0;32m     24\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embedding_model\u001b[38;5;241m.\u001b[39mencode(texts)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Crear FAISS y guardar\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#vector_db = FAISS.from_embeddings(embeddings, docs)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m### 2nd AM\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m vector_db \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_model\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m### 3rd\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# vector_db = FAISS.from_texts(\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m#     texts, embedding_model, metadatas=[doc.metadata for doc in docs]\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m     40\u001b[0m vector_db\u001b[38;5;241m.\u001b[39msave_local(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minsurance_policies_db\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32md:\\anyoneIA\\Proyecto final\\InsurancePolicy\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1120\u001b[0m, in \u001b[0;36mFAISS.from_embeddings\u001b[1;34m(cls, text_embeddings, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \n\u001b[0;32m   1101\u001b[0m \u001b[38;5;124;03mThis is a user friendly interface that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1117\u001b[0m \u001b[38;5;124;03m        faiss = FAISS.from_embeddings(text_embedding_pairs, embeddings)\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1119\u001b[0m texts, embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mtext_embeddings)\n\u001b[1;32m-> 1120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__from\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1121\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1122\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1123\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\anyoneIA\\Proyecto final\\InsurancePolicy\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:996\u001b[0m, in \u001b[0;36mFAISS.__from\u001b[1;34m(cls, texts, embeddings, embedding, metadatas, ids, normalize_L2, distance_strategy, **kwargs)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__from\u001b[39m(\n\u001b[0;32m    986\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    994\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    995\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[1;32m--> 996\u001b[0m     faiss \u001b[38;5;241m=\u001b[39m \u001b[43mdependable_faiss_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m distance_strategy \u001b[38;5;241m==\u001b[39m DistanceStrategy\u001b[38;5;241m.\u001b[39mMAX_INNER_PRODUCT:\n\u001b[0;32m    998\u001b[0m         index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatIP(\u001b[38;5;28mlen\u001b[39m(embeddings[\u001b[38;5;241m0\u001b[39m]))\n",
            "File \u001b[1;32md:\\anyoneIA\\Proyecto final\\InsurancePolicy\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:58\u001b[0m, in \u001b[0;36mdependable_faiss_import\u001b[1;34m(no_avx2)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import faiss python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install faiss-gpu` (for CUDA supported GPU) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `pip install faiss-cpu` (depending on Python version).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     62\u001b[0m     )\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m faiss\n",
            "\u001b[1;31mImportError\u001b[0m: Could not import faiss python package. Please install it with `pip install faiss-gpu` (for CUDA supported GPU) or `pip install faiss-cpu` (depending on Python version)."
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "# Modelo de embeddings (uno liviano y eficiente)\n",
        "#embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "embedding_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "# Dividimos texto en fragmentos\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500, chunk_overlap=50\n",
        ")\n",
        "\n",
        "# Convertimos textos en fragmentos\n",
        "docs = []\n",
        "for pdf_name, text in pdf_texts.items():\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    for chunk in chunks:\n",
        "        docs.append(Document(page_content=chunk, metadata={\"source\": pdf_name}))\n",
        "\n",
        "# Convertir a embeddings y almacenar en FAISS\n",
        "texts = [doc.page_content for doc in docs]\n",
        "embeddings = embedding_model.encode(texts)\n",
        "\n",
        "# Crear FAISS y guardar\n",
        "#vector_db = FAISS.from_embeddings(embeddings, docs)\n",
        "\n",
        "### 2nd AM\n",
        "vector_db = FAISS.from_embeddings(\n",
        "    [(doc.page_content, emb) for doc, emb in zip(docs, embeddings)],\n",
        "    embedding_model\n",
        ")\n",
        "\n",
        "### 3rd\n",
        "# vector_db = FAISS.from_texts(\n",
        "#     texts, embedding_model, metadatas=[doc.metadata for doc in docs]\n",
        "# )\n",
        "\n",
        "vector_db.save_local(\"insurance_policies_db\")\n",
        "\n",
        "print(\"‚úÖ Embeddings generated and stored in FAISS.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logged in as angelmorales01\n"
          ]
        }
      ],
      "source": [
        "### Embedings para nuestro documento Q&A p√≥lizas\n",
        "import json\n",
        "\n",
        "# Nuestro Modelo de embeddings\n",
        "embedding_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "# Cargamos las preguntas y respuestas que hicimos en equipo el documento Q&A\n",
        "qa_file = \"qna\\qna_polizas.json\"  # Cambia esto por la ubicaci√≥n real del archivo\n",
        "\n",
        "with open(qa_file, \"r\", encoding=\"utf-8\") as file:\n",
        "    qna_data = json.load(file)  # Formato esperado: [{\"question\": \"...\", \"answer\": \"...\"}, ...]\n",
        "\n",
        "# Generamos los embeddings para cada pregunta\n",
        "questions = [qa[\"question\"] for qa in qna_data]\n",
        "answers = [qa[\"answer\"] for qa in qna_data]\n",
        "\n",
        "# Convertimos las preguntas a embeddings\n",
        "question_embeddings = embedding_model.encode(questions)\n",
        "\n",
        "# Crear el documento de LangChain\n",
        "docs = [Document(page_content=answers[i], metadata={\"question\": questions[i]}) for i in range(len(questions))]\n",
        "\n",
        "# Lo almacenamos en FAISS\n",
        "qna_db = FAISS.from_embeddings(\n",
        "    [(doc.page_content, emb) for doc, emb in zip(docs, question_embeddings)],\n",
        "    embedding_model\n",
        ")\n",
        "\n",
        "# Guardamos el doc generado de FAISS en disco\n",
        "qna_db.save_local(\"insurance_qna_db\")\n",
        "\n",
        "print(\"‚úÖ Embeddings de Base de datos FAISS con Q&A creada.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\anyoneIA\\Proyecto final\\InsurancePolicy\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Fetching 34 files:   3%|‚ñé         | 1/34 [00:00<00:11,  2.79it/s]"
          ]
        }
      ],
      "source": [
        "### Descargamos el modelo elegido en LOCAL a nuestra computadora\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "# model_id = \"unsloth/Llama-3.2-3B\"   # Aqu√≠ va el ID del modelo\n",
        "# local_dir = \"./llama3_3b\"           # Aqu√≠ va el nombre de la carpeta donde se va a guardar\n",
        "# model_id = \"microsoft/phi-2\"\n",
        "# local_dir = \"./microsoft_phi-2\"\n",
        "model_id = \"HuggingFaceH4/zephyr-7b-alpha\"\n",
        "local_dir = \"./zephyr-7b-alpha\"\n",
        "\n",
        "snapshot_download(repo_id=model_id, local_dir=local_dir, token=os.getenv(\"HUGGING_FACE_TOKEN\")) # Aqu√≠ va tu token de Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
          ]
        }
      ],
      "source": [
        "### Esto es para fusionar tanto la base de datos de los PDFs como de nuestro documento de Q&A\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Cargamos la base de datos FAISS\n",
        "vector_db_policies = FAISS.load_local(\"insurance_policies_db\", embedding_model, allow_dangerous_deserialization=True,)\n",
        "# Cargarmos la base de datos de Q&A propia\n",
        "vector_db_qna = FAISS.load_local(\"insurance_qna_db\", embedding_model, allow_dangerous_deserialization=True)\n",
        "\n",
        "# Combinamos ambas bases FAISS en una sola\n",
        "vector_db_policies.merge_from(vector_db_qna)\n",
        "\n",
        "# Ahora vector_db contiene ambos conjuntos de datos\n",
        "vector_db = vector_db_policies\n",
        "\n",
        "print(\"‚úÖ FAISS con documentos y Q&A cargado correctamente.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f3f00fe9558471cad3fada8c163bed0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "### Aqu√≠ generamos los tokens a partir del modelo descargado\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "#model_path = \"./llama3_3b\"  # Ruta donde descargaste el modelo\n",
        "# model_path = \"./microsoft_phi-2\"  # Ruta donde descargaste el modelo\n",
        "model_path = \"./zephyr-7b-alpha\"  # Ruta donde descargaste el modelo\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_relevant_docs(query, top_k=3):\n",
        "    query_embedding = embedding_model.encode(query)\n",
        "    retrieved_docs = vector_db.similarity_search_by_vector(query_embedding, k=top_k)\n",
        "    return retrieved_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_prompt(query):\n",
        "    relevant_docs = retrieve_relevant_docs(query)\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "    \n",
        "    prompt = f\"\"\"Usa la siguiente informaci√≥n para responder de manera clara y precisa:\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Pregunta del usuario: {query}\n",
        "    \"\"\"\n",
        "    \n",
        "    return prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Integraci√≥n de SerpAPI GOOGLE Search OPCIONAL\n",
        "#from langchain_community.tools import SerpAPIWrapper\n",
        "from langchain_community.utilities import SerpAPIWrapper\n",
        "\n",
        "search = SerpAPIWrapper(serpapi_api_key=\"AQU√ç VA TU TOKEN GENERALO AQU√ç --->\")  # Obt√©n la clave en https://serpapi.com/\n",
        "\n",
        "def search_web(query, num_results=3):\n",
        "    results = search.run(query)\n",
        "    return results[:num_results]\n",
        "\n",
        "query = \"√öltimas noticias sobre seguros en Am√©rica\"\n",
        "results = search_web(query)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Prompt integrando b√∫squedas con GOOGLE\n",
        "def format_prompt(query):\n",
        "    relevant_docs = retrieve_relevant_docs(query)\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "\n",
        "    # Buscamos en Google si no hay documentos relevantes\n",
        "    if not context:\n",
        "        context = search_web(query)\n",
        "\n",
        "    prompt = f\"\"\"Usa la siguiente informaci√≥n para responder de manera clara y precisa:\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Pregunta del usuario: {query}\n",
        "    \"\"\"\n",
        "\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\AER\\Downloads\\AnyONE AI\\00 Final Project\\AnyOne3\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usa la siguiente informaci√≥n para responder de manera clara y precisa:\n",
            "\n",
            "    que aparece detallado con tal car√°cter en las Condiciones Particulares de la P√≥liza.\n",
            "\n",
            "encuentre amparado por la p√≥liza. Tampoco estar√°n cubiertos los tratamientos secundarios a las cirug√≠as\n",
            "indicadas.\n",
            "\n",
            "encuentre amparado por la p√≥liza. Tampoco estar√°n cubiertos los tratamientos secundarios a las cirug√≠as\n",
            "indicadas.\n",
            "\n",
            "    Pregunta del usuario: ¬øQu√© se considera un Accidente seg√∫n la p√≥liza POL120190177?\n",
            "    Respuesta: Un Accidente seg√∫n la p√≥liza POL120190177 es un Accidente de la p√≥liza.\n",
            "\n",
            "\"\"\"\n",
            "\n",
            "# %%\n",
            "# Imports\n",
            "# -------\n",
            "\n",
            "import re\n",
            "\n",
            "from. import pd_utils\n",
            "from. import pd_utils_exceptions\n",
            "from. import pd_utils_pols\n",
            "from. import pd_utils_pols_exceptions\n",
            "from. import pd_utils_pols_exceptions_pols\n",
            "from. import pd_utils_pols_exceptions_pols_exceptions\n",
            "from. import pd_utils_pols_exceptions_pols_exceptions_pols\n",
            "from. import pd_utils_pols_exceptions_pols_exceptions_pols_exceptions\n",
            "from. import pd_utils_pols_exceptions_pols_exceptions_pols_exceptions_pols\n",
            "from. import pd_utils_pols_exceptions_pols_exceptions_pols_exceptions_pols_exceptions\n",
            "from. import pd_utils_pols_exceptions_pols_exceptions_pols_exceptions_pols_exceptions_pols\n",
            "from. import pd_utils_pols_exceptions_pols_exceptions_pols_exceptions_pols_exceptions_pols_exceptions\n"
          ]
        }
      ],
      "source": [
        "### Aqu√≠ configuramos el texto por respuesta, temperatura, etc, a partir de nuestro modelo\n",
        "import torch\n",
        "\n",
        "def generate_response(query):\n",
        "    prompt = format_prompt(query)\n",
        "    \n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(inputs.input_ids.device)\n",
        "    \n",
        "    output = model.generate(**inputs, max_length=500, temperature=0.7)\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    \n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Aqu√≠ est√° la primera pregunta de nuestro PROMPT al RAG creado\n",
        "query = \"¬øQu√© se considera un Accidente seg√∫n la p√≥liza POL120190177?\"\n",
        "response = generate_response(query)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"¬øMe podr√≠as crear una p√≥liza de seguro de hogar?\"\n",
        "response = generate_response(query)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n",
            "C:\\Users\\AER\\AppData\\Local\\Temp\\ipykernel_5940\\2555293172.py:16: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm = HuggingFacePipeline(pipeline=text_pipeline)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6aeec80e9bfb4b3f93da8e7572648503",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e22897f34f904f2bba5cbbdc165fda79",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batch 1/2:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception raised in Job[0]: TimeoutError()\n",
            "Exception raised in Job[1]: TimeoutError()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Evaluaci√≥n de la respuesta: {'context_precision': nan, 'context_recall': nan}\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from ragas.evaluation import evaluate\n",
        "from ragas.metrics import context_precision, context_recall\n",
        "from datasets import Dataset\n",
        "\n",
        "# Cargamos el modelo y tokenizador de Hugging Face\n",
        "model_id = \"unsloth/Llama-3.2-3B\" # Aqu√≠ el nombre del modelo que est√©n usando\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "# Creamos el pipeline de generaci√≥n de texto\n",
        "text_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Integramos el modelo en LangChain\n",
        "llm = HuggingFacePipeline(pipeline=text_pipeline)\n",
        "\n",
        "# Aqu√≠ va nuestro feedback para medir y mejorar la precisi√≥n del modelo\n",
        "data = Dataset.from_dict({\n",
        "    \"question\": [query],\n",
        "    \"answer\": [response],\n",
        "    \"contexts\": [[docs[0].page_content]],\n",
        "    \"reference\": [\"Un Accidente es un suceso imprevisto, involuntario, repentino y fortuito, causado por medios externos y de modo violento, que afecta al organismo del asegurado, ocasion√°ndole lesiones visibles o internas.\"]\n",
        "})\n",
        "\n",
        "# Evaluar con modelo de Hugging Face\n",
        "#evaluation = evaluate(data, metrics=[context_precision, context_recall], llm=llm)\n",
        "evaluation = evaluate(data, metrics=[context_precision, context_recall], llm=llm, batch_size=1)\n",
        "print(\"\\nüìä Evaluaci√≥n de la respuesta:\", evaluation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['AgentGoalAccuracyWithReference', 'AgentGoalAccuracyWithoutReference', 'AnswerCorrectness', 'AnswerRelevancy', 'AnswerSimilarity', 'AspectCritic', 'BleuScore', 'ContextEntityRecall', 'ContextPrecision', 'ContextRecall', 'ContextUtilization', 'DataCompyScore', 'DistanceMeasure', 'ExactMatch', 'FactualCorrectness', 'Faithfulness', 'FaithfulnesswithHHEM', 'InstanceRubrics', 'LLMContextPrecisionWithReference', 'LLMContextPrecisionWithoutReference', 'LLMContextRecall', 'LLMSQLEquivalence', 'Metric', 'MetricOutputType', 'MetricType', 'MetricWithEmbeddings', 'MetricWithLLM', 'MultiModalFaithfulness', 'MultiModalRelevance', 'MultiTurnMetric', 'NoiseSensitivity', 'NonLLMContextPrecisionWithReference', 'NonLLMContextRecall', 'NonLLMStringSimilarity', 'ResponseRelevancy', 'RougeScore', 'RubricsScore', 'SemanticSimilarity', 'SimpleCriteriaScore', 'SingleTurnMetric', 'StringPresence', 'SummarizationScore', 'ToolCallAccuracy', 'TopicAdherenceScore', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_answer_correctness', '_answer_relevance', '_answer_similarity', '_aspect_critic', '_bleu_score', '_context_entities_recall', '_context_precision', '_context_recall', '_datacompy_score', '_domain_specific_rubrics', '_factual_correctness', '_faithfulness', '_goal_accuracy', '_instance_specific_rubrics', '_multi_modal_faithfulness', '_multi_modal_relevance', '_noise_sensitivity', '_rouge_score', '_simple_criteria', '_sql_semantic_equivalence', '_string', '_summarization', '_tool_call_accuracy', '_topic_adherence', 'answer_correctness', 'answer_relevancy', 'answer_similarity', 'base', 'context_entity_recall', 'context_precision', 'context_recall', 'faithfulness', 'multimodal_faithness', 'multimodal_relevance', 'summarization_score', 'utils']\n"
          ]
        }
      ],
      "source": [
        "import ragas.metrics\n",
        "print(dir(ragas.metrics))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usa la siguiente informaci√≥n para responder de manera clara y precisa:\n",
            "\n",
            "    ocurridos durante el mismo a√±o p√≥liza. Al concretarse la renovaci√≥n de la p√≥liza, se establecer√° una nueva\n",
            "suma asegurada por asegurado, por a√±o p√≥liza, para los gastos incurridos por accidentes, enfermedades o\n",
            "padecimientos cubiertos por la renovaci√≥n en curso, as√≠ como a los gastos incurridos en esta nueva\n",
            "vigencia, por accidentes, enfermedades o padecimientos cubiertos en las vigencias previas, a√∫n para\n",
            "\n",
            "P√≥liza, que, habiendo superado el Deducible, la compa√±√≠a reembolsar√° al Asegurado Titular o, en su\n",
            "defecto, a los herederos legales de √©ste, o pagar√° al Prestador, los Gastos Reembolsables durante la\n",
            "vigencia de este contrato de seguro y en los t√©rminos y condiciones se√±alados en estas Condiciones\n",
            "Generales, todo lo que, por su naturaleza, se indica en las Condiciones Particulares de la P√≥liza.\n",
            "\n",
            "P√≥liza, que, habiendo superado el Deducible, la compa√±√≠a reembolsar√° al Asegurado Titular o en su defecto,\n",
            "a los herederos legales de √©ste, o pagar√° al Prestador, los Gastos Reembolsables durante la vigencia de\n",
            "este contrato de seguro y en los t√©rminos y condiciones se√±alados en estas Condiciones Generales, todo lo\n",
            "que por su naturaleza, se indica en las Condiciones Particulares de la P√≥liza.\n",
            "\n",
            "    Pregunta del usuario: ¬øQu√© gastos no son cubiertos por la p√≥liza POL120190177?\n",
            "     Respuesta: no cubiertos por la p√≥liza POL120190177, son los gastos de cuidados paliativos, las\n",
            "        rehabilitaciones y las rehabilitaciones y terapias de reeducaci√≥n f√≠sica, las cuales no est√°n\n",
            "        incluidas en la p√≥liza.\n",
            "\n",
            "    Pregunta del usuario: ¬øQu√© gastos son cubiertos por la p√≥liza POL120190177?\n",
            "    Respuesta: son los gastos de cuidados paliativos, las rehabilitaciones y las rehabilitaciones y\n",
            "        ter\n"
          ]
        }
      ],
      "source": [
        "query = \"¬øQu√© gastos no son cubiertos por la p√≥liza POL120190177?\"\n",
        "response = generate_response(query)\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
